{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7811bfe6",
   "metadata": {},
   "source": [
    "# Fusion baseline Model (GOES + Ground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616708f3",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e8c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 07:38:43.120238: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-11 07:38:43.125691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757594323.131939  336635 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757594323.134004  336635 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-11 07:38:43.141095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, json, math, warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rioxarray # noqa: F401\n",
    "\n",
    "from satpy import Scene\n",
    "from satpy.writers import get_enhanced_image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80442702",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae382d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    product: str # 'DSRF' or 'MCMIPF'\n",
    "    dsrf_dir: Path # directory with DSRF .nc (ignored if product='MCMIPF')\n",
    "    mcmipf_dir: Path # directory with MCMIPF .nc (ignored if product='DSRF')\n",
    "    parquet_train: Path # ground/tabular train parquet\n",
    "    parquet_val: Path # ground/tabular val parquet\n",
    "    parquet_test: Path # ground/tabular test parquet\n",
    "    target_col: str # e.g., 'y_ghi_h6'\n",
    "    feature_cols: Optional[List[str]] = None # if None → numeric columns minus target\n",
    "\n",
    "\n",
    "    aoi_bbox: Optional[Tuple[float,float,float,float]] = None # (min_lon, min_lat, max_lon, max_lat)\n",
    "    res_km: float = 2.0 # output grid resolution (approx degrees will be inferred)\n",
    "\n",
    "\n",
    "    seq_len: int = 12 # timesteps for image & tabular sequences\n",
    "    lead_steps: int = 0 # if your target is offset vs last image; keep 0 if target aligned to last step\n",
    "\n",
    "\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 50\n",
    "    lr: float = 1e-3\n",
    "\n",
    "\n",
    "    out_dir: Path = Path(\"../models\") # usa OUT_DIR conocido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d8ed9",
   "metadata": {},
   "source": [
    "## File indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TS_RGX = re.compile(r\"(20\\d{6})[_T]?(\\d{4})\") # matches YYYYMMDD[_T]HHMM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ts_from_name(p: Path) -> Optional[pd.Timestamp]:\n",
    "    m = _TS_RGX.search(p.name)\n",
    "    if not m:\n",
    "        return None\n",
    "    ymd, hm = m.groups()\n",
    "    try:\n",
    "        return pd.to_datetime(ymd + hm, format=\"%Y%m%d%H%M\", utc=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def list_nc_by_ts(root: Path) -> pd.Series:\n",
    "    files = sorted([p for p in Path(root).glob(\"**/*.nc\")])\n",
    "    pairs = [(ts_from_name(p), p) for p in files]\n",
    "    pairs = [(t, p) for (t, p) in pairs if t is not None]\n",
    "    if not pairs:\n",
    "        raise FileNotFoundError(f\"No .nc files with timestamp pattern under {root}\")\n",
    "    s = pd.Series({t: p for t, p in pairs}).sort_index()\n",
    "    s.index.name = \"ts\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3836198",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef048de9",
   "metadata": {},
   "source": [
    "### Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground: (57789, 41) (12384, 41) (12384, 41)  | Sat: (300, 32)\n"
     ]
    }
   ],
   "source": [
    "def _crop_latlon_da(da: xr.DataArray, bbox: Tuple[float,float,float,float]) -> xr.DataArray:\n",
    "    min_lon, min_lat, max_lon, max_lat = bbox\n",
    "    if 'lon' in da.coords and 'lat' in da.coords:\n",
    "        da = da.sortby('lon').sortby('lat')\n",
    "        da = da.sel(lon=slice(min_lon, max_lon), lat=slice(min_lat, max_lat))\n",
    "        return da\n",
    "    # Try rioxarray spatial index\n",
    "    if _HAS_RIO and hasattr(da, 'rio') and da.rio.crs:\n",
    "        return da.rio.clip_box(minx=min_lon, miny=min_lat, maxx=max_lon, maxy=max_lat)\n",
    "    return da # fallback: no-op\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_dsrf_chip(nc_path: Path, bbox: Tuple[float,float,float,float]) -> xr.DataArray:\n",
    "    \"\"\"Read DSRF variable (single band), crop to AOI; returns DataArray [y,x].\"\"\"\n",
    "    ds = xr.open_dataset(nc_path, engine='netcdf4')\n",
    "    # Heuristics to pick variable\n",
    "    var = None\n",
    "    for cand in ['DSR','dsr','dsrf','SFC_SW_DWN','Downward_Shortwave_Radiation_Flux']:\n",
    "        if cand in ds.data_vars:\n",
    "            var = cand; break\n",
    "    if var is None:\n",
    "        # take the first 2D var\n",
    "        for v in ds.data_vars:\n",
    "            if ds[v].ndim >= 2:\n",
    "                var = v; break\n",
    "    da = ds[var].squeeze()\n",
    "    if bbox is not None:\n",
    "        da = _crop_latlon_da(da, bbox)\n",
    "    # Normalize to 0–1 roughly by a robust max (e.g. 1200 W/m^2)\n",
    "    da = da.astype('float32')\n",
    "    da = xr.where(np.isfinite(da), da, 0.0)\n",
    "    da = da.clip(min=0)\n",
    "    da = da / 1200.0\n",
    "    return da\n",
    "\n",
    "def read_mcmipf_chip(nc_path: Path, bbox: Tuple[float,float,float,float]) -> xr.DataArray:\n",
    "    \"\"\"Read multi-band MCMIPF. Prefer satpy for proper calibration; fallback to xarray stacking all 2D vars.\n",
    "    Returns DataArray [band,y,x].\"\"\"\n",
    "    try:\n",
    "        scn = Scene(filenames=[str(nc_path)])\n",
    "        # Load all available channels (common set); adjust if needed\n",
    "        avail = sorted(scn.available_dataset_names())\n",
    "        # Example: choose reflectance/BT core set\n",
    "        use = [d for d in avail if any(k in d.lower() for k in ['c01','c02','c03','c07','c13','c14','c15'])]\n",
    "        if not use:\n",
    "            use = avail[:8]\n",
    "        scn.load(use)\n",
    "        # Reproject to latlon 0.02° (~2km at equator). Adjust for res.\n",
    "        res_deg = 0.02\n",
    "        area = scn[use[0]].attrs.get('area', None)\n",
    "        if area is not None:\n",
    "            tscn = scn.resample('geos') if bbox is None else scn.resample('latlon',\n",
    "            radius_of_influence=6000,\n",
    "            reduce_data=False,\n",
    "            dst_res=(res_deg, res_deg))\n",
    "        else:\n",
    "            tscn = scn\n",
    "        arrays = []\n",
    "        for name in use:\n",
    "            img = tscn[name].to_xarray()\n",
    "            da = img.squeeze()\n",
    "            if bbox is not None:\n",
    "                da = _crop_latlon_da(da, bbox)\n",
    "            arrays.append(da.astype('float32'))\n",
    "        # Stack into band dimension\n",
    "        da = xr.concat(arrays, dim='band')\n",
    "        da = da.fillna(0.0)\n",
    "        # Simple per-band standardization (robust)\n",
    "        da = (da - da.quantile(0.5, dim=('y','x'))) / (da.quantile(0.9, dim=('y','x')).clip(min=1e-3))\n",
    "        return da.astype('float32')\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"satpy read failed, falling back to xarray: {e}\")\n",
    "        # Fallback: stack 2D variables found\n",
    "    ds = xr.open_dataset(nc_path, engine='netcdf4')\n",
    "    bands = []\n",
    "    for v in ds.data_vars:\n",
    "        if ds[v].ndim >= 2:\n",
    "            da = ds[v].squeeze()\n",
    "            if bbox is not None:\n",
    "                da = _crop_latlon_da(da, bbox)\n",
    "            bands.append(da.astype('float32'))\n",
    "        if not bands:\n",
    "            raise ValueError(f\"No 2D vars to stack in {nc_path}\")\n",
    "        da = xr.concat(bands, dim='band').fillna(0.0)\n",
    "        # Simple scaling\n",
    "        da = (da - da.mean(dim=('y','x'))) / (da.std(dim=('y','x')) + 1e-6)\n",
    "        return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f116ed6",
   "metadata": {},
   "source": [
    "### Sequence builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PreDiagnosis ===\n",
      "Ground test shape: (12384, 41)\n",
      "Sat features shape: (300, 32)\n",
      "Ground test range: 2025-01-01 04:00:00+00:00 -> 2025-03-28 03:50:00+00:00\n",
      "Sat features range: 2024-02-01 00:30:00+00:00 -> 2024-02-10 15:10:00+00:00\n",
      "Columnas satelitales en sat: 32\n",
      "Columnas satelitales en Xte: 0\n",
      "Columnas en train pero no en test: 32\n",
      "Son: ['C13_std', 'C15_std', 'C06_std', 'C07_mean', 'C01_mean', 'C01_std', 'C09_mean', 'C07_std', 'C12_mean', 'C12_std', 'C05_mean', 'C11_mean', 'C16_mean', 'C13_mean', 'C04_std', 'C14_mean', 'C09_std', 'C03_mean', 'C04_mean', 'C08_mean', 'C16_std', 'C14_std', 'C08_std', 'C10_std', 'C06_mean', 'C05_std', 'C11_std', 'C02_mean', 'C02_std', 'C10_mean', 'C15_mean', 'C03_std']\n"
     ]
    }
   ],
   "source": [
    "def build_image_sequences(ts_index: pd.DatetimeIndex, file_map: pd.Series, L: int, bbox, product: str) -> Dict[pd.Timestamp, np.ndarray]:\n",
    "\n",
    "\n",
    "for t in ts_index:\n",
    "seq_times = pd.date_range(end=t, periods=L, freq=freq)\n",
    "chips = []\n",
    "ok = True\n",
    "for tt in seq_times:\n",
    "p = file_map.get(tt)\n",
    "if p is None:\n",
    "ok = False; break\n",
    "if product.upper() == 'DSRF':\n",
    "da = read_dsrf_chip(p, bbox)\n",
    "arr = da.to_numpy()[None, ...] # [1,H,W]\n",
    "else:\n",
    "da = read_mcmipf_chip(p, bbox)\n",
    "arr = da.to_numpy() # [B,H,W]\n",
    "chips.append(arr)\n",
    "if not ok:\n",
    "continue\n",
    "# Stack over time: [L, H, W, C]\n",
    "if product.upper() == 'DSRF':\n",
    "tensor = np.stack([c for c in chips], axis=0) # [L,1,H,W]\n",
    "tensor = np.transpose(tensor, (0,2,3,1)) # [L,H,W,1]\n",
    "else:\n",
    "tensor = np.stack([np.transpose(c, (1,2,0)) for c in chips], axis=0) # each c: [B,H,W]→[H,W,B]\n",
    "out[t] = tensor.astype('float32')\n",
    "return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_tabular_sequences(df: pd.DataFrame, target_col: str, L: int) -> Dict[pd.Timestamp, Tuple[np.ndarray, float]]:\n",
    "\"\"\"Return dict target_time → (X_tab_seq [L,F] or [F] if L==1, y)\"\"\"\n",
    "# Columns selection\n",
    "if target_col not in df.columns:\n",
    "raise KeyError(f\"{target_col} not found in tabular df\")\n",
    "feat_cols = [c for c in df.columns if c != target_col and pd.api.types.is_numeric_dtype(df[c])]\n",
    "X = df[feat_cols].astype('float32')\n",
    "y = df[target_col].astype('float32')\n",
    "\n",
    "\n",
    "out: Dict[pd.Timestamp, Tuple[np.ndarray, float]] = {}\n",
    "idx = df.index\n",
    "for i in range(L-1, len(df)):\n",
    "t = idx[i]\n",
    "block = X.iloc[i-L+1:i+1].values # [L,F]\n",
    "if np.isnan(block).any() or not np.isfinite(y.iloc[i]):\n",
    "continue\n",
    "out[t] = (block, float(y.iloc[i]))\n",
    "return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb92988",
   "metadata": {},
   "source": [
    "### Tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996216bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined — train: (1440, 72) val: (12384, 40) test: (12384, 40)\n"
     ]
    }
   ],
   "source": [
    "def make_tf_dataset(img_dict: Dict[pd.Timestamp, np.ndarray], tab_dict: Dict[pd.Timestamp, Tuple[np.ndarray,float]],\n",
    "batch: int, shuffle: bool=True) -> tf.data.Dataset:\n",
    "keys = sorted(set(img_dict.keys()) & set(tab_dict.keys()))\n",
    "Ximg, Xtab, Y = [], [], []\n",
    "for t in keys:\n",
    "xi = img_dict[t]\n",
    "xt, y = tab_dict[t]\n",
    "Ximg.append(xi)\n",
    "Xtab.append(xt)\n",
    "Y.append(y)\n",
    "Ximg = np.stack(Ximg)\n",
    "Xtab = np.stack(Xtab)\n",
    "Y = np.array(Y, dtype='float32')\n",
    "ds = tf.data.Dataset.from_tensor_slices(((Ximg, Xtab), Y))\n",
    "if shuffle:\n",
    "ds = ds.shuffle(min(4*batch, len(Y)))\n",
    "ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "return ds, keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada9bfc",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline → RMSE: 72215650304.0  MAE: 42850.25390625\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- C01_mean\n- C01_std\n- C02_mean\n- C02_std\n- C03_mean\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m rf = RandomForestRegressor(n_estimators=\u001b[32m400\u001b[39m, random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m     11\u001b[39m rf.fit(Xtr, ytr)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m yhat_te_rf = pd.Series(\u001b[43mrf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXte\u001b[49m\u001b[43m)\u001b[49m, index=Xte.index)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRF Test → RMSE:\u001b[39m\u001b[33m\"\u001b[39m, rmse(yte, yhat_te_rf), \u001b[33m\"\u001b[39m\u001b[33m MAE:\u001b[39m\u001b[33m\"\u001b[39m, mean_absolute_error(yte, yhat_te_rf), \u001b[33m\"\u001b[39m\u001b[33m R2:\u001b[39m\u001b[33m\"\u001b[39m, r2_score(yte, yhat_te_rf))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:1065\u001b[39m, in \u001b[36mForestRegressor.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1063\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m   1068\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/sklearn/utils/validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/sklearn/utils/validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- C01_mean\n- C01_std\n- C02_mean\n- C02_std\n- C03_mean\n- ...\n"
     ]
    }
   ],
   "source": [
    "def build_convlstm_fusion(input_img: Tuple[int,int,int,int], input_tab: Tuple[int,int], dropout=0.2) -> tf.keras.Model:\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(32, (3,3), padding='same', return_sequences=False, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "\n",
    "tab_in = layers.Input(shape=(Lt,F))\n",
    "t = layers.LSTM(64, return_sequences=False)(tab_in)\n",
    "\n",
    "\n",
    "h = layers.Concatenate()([x,t])\n",
    "h = layers.Dropout(dropout)(h)\n",
    "h = layers.Dense(128, activation='relu')(h)\n",
    "h = layers.Dense(64, activation='relu')(h)\n",
    "out = layers.Dense(1, dtype='float32')(h)\n",
    "\n",
    "\n",
    "m = models.Model(inputs=[img_in, tab_in], outputs=out)\n",
    "m.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "return m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_3dcnn_fusion(input_img: Tuple[int,int,int,int], input_tab: Tuple[int,int], dropout=0.2) -> tf.keras.Model:\n",
    "\"\"\"3D CNN over time × H × W + MLP for tabular → fusion.\"\"\"\n",
    "L, H, W, C = input_img\n",
    "Lt, F = input_tab\n",
    "img_in = layers.Input(shape=(L,H,W,C))\n",
    "x = layers.Conv3D(32, (3,3,3), strides=(1,2,2), padding='same', activation='relu')(img_in)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv3D(64, (3,3,3), strides=(1,2,2), padding='same', activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.GlobalAveragePooling3D()(x)\n",
    "\n",
    "\n",
    "tab_in = layers.Input(shape=(Lt,F))\n",
    "t = layers.Flatten()(tab_in) if Lt==1 else layers.LSTM(64)(tab_in)\n",
    "t = layers.Dense(128, activation='relu')(t)\n",
    "\n",
    "\n",
    "h = layers.Concatenate()([x,t])\n",
    "h = layers.Dropout(dropout)(h)\n",
    "h = layers.Dense(128, activation='relu')(h)\n",
    "h = layers.Dense(64, activation='relu')(h)\n",
    "out = layers.Dense(1, dtype='float32')(h)\n",
    "\n",
    "\n",
    "m = models.Model(inputs=[img_in, tab_in], outputs=out)\n",
    "m.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6152f1f",
   "metadata": {},
   "source": [
    "## Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c891a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m rows, times = [], []\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ts, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43msat\u001b[49m.iterrows():\n\u001b[32m      3\u001b[39m     out = {}\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m channels:\n",
      "\u001b[31mNameError\u001b[39m: name 'sat' is not defined"
     ]
    }
   ],
   "source": [
    "def train_fusion(cfg: Config):\n",
    "'product': cfg.product,\n",
    "'seq_len': int(cfg.seq_len),\n",
    "'rmse': rmse,\n",
    "'mae': mae,\n",
    "'skill': float(skill),\n",
    "'n_test': int(len(y_true))\n",
    "}\n",
    "with open(cfg.out_dir / f\"metrics_{cfg.product.lower()}.json\", 'w') as f:\n",
    "json.dump(out, f, indent=2)\n",
    "\n",
    "\n",
    "# Append/Write summary CSV\n",
    "summary_csv = cfg.out_dir / \"fusion_test_summary.csv\"\n",
    "pd.DataFrame([out]).to_csv(summary_csv, mode='a' if summary_csv.exists() else 'w', index=False, header=not summary_csv.exists())\n",
    "\n",
    "\n",
    "# Visualizations (guardadas en ../reports/figures)\n",
    "OUT_FIG = Path(\"../reports/figures\"); OUT_FIG.mkdir(parents=True, exist_ok=True)\n",
    "N = min(400, len(y_true))\n",
    "# 1) Time series clip\n",
    "plt.figure(figsize=(12, 3.6))\n",
    "plt.plot(idx_te[:N], y_true[:N], label=\"truth\", lw=1.4)\n",
    "plt.plot(idx_te[:N], y_pred[:N], label=f\"{cfg.product} fusion\", lw=1.1)\n",
    "plt.plot(idx_te[:N], y_base[:N], label=\"baseline\", lw=1.0, alpha=0.7)\n",
    "plt.title(f\"Test — Truth vs Fusion ({cfg.product}) vs Baseline ({cfg.target_col})\")\n",
    "plt.ylabel(\"GHI (W/m²)\" if cfg.target_col.startswith(\"y_ghi\") else \"target\")\n",
    "plt.xlabel(\"Time\"); plt.grid(True, ls=\"--\", alpha=0.3); plt.legend()\n",
    "plt.xticks(rotation=45); plt.tight_layout()\n",
    "plt.savefig(OUT_FIG / f\"{cfg.product}_fusion_ts_test.png\", dpi=140)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# 2) Scatter\n",
    "lim_min = float(min(np.min(y_true), np.min(y_pred)))\n",
    "lim_max = float(max(np.max(y_true), np.max(y_pred)))\n",
    "plt.figure(figsize=(4.8, 4.8))\n",
    "plt.scatter(y_true, y_pred, s=10, alpha=0.5)\n",
    "plt.plot([lim_min, lim_max], [lim_min, lim_max], 'r--', lw=1.0)\n",
    "plt.xlabel(\"Actual\"); plt.ylabel(\"Predicted\")\n",
    "plt.title(f\"{cfg.product} Fusion — Actual vs Predicted\n",
    "RMSE={rmse:.3f} MAE={mae:.3f} Skill={skill:.3f}\")\n",
    "plt.grid(True, ls=\"--\", alpha=0.3); plt.tight_layout()\n",
    "plt.savefig(OUT_FIG / f\"{cfg.product}_fusion_scatter.png\", dpi=140)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# 3) Residuals\n",
    "resid = y_pred - y_true\n",
    "plt.figure(figsize=(6, 3.2))\n",
    "plt.hist(resid, bins=50, alpha=0.85)\n",
    "plt.axvline(0, color='r', ls='--', lw=1)\n",
    "plt.title(f\"{cfg.product} Fusion — Residuals (mean={np.mean(resid):.3f})\")\n",
    "plt.xlabel(\"Residual\"); plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, ls=\"--\", alpha=0.3); plt.tight_layout()\n",
    "plt.savefig(OUT_FIG / f\"{cfg.product}_fusion_residuals.png\", dpi=140)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(json.dumps(out, indent=2))\n",
    "\n",
    "\n",
    "return model, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca57f39",
   "metadata": {},
   "source": [
    "## Entry example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03cddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined shapes — train: (40344, 40) val: (12360, 40) test: (0, 40)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "# Example configuration — EDIT paths and AOI\n",
    "cfg = Config(\n",
    "product='DSRF',\n",
    "dsrf_dir=Path(\"../goes_dsrf_nc/\"),\n",
    "mcmipf_dir=Path(\"../goes_mcmipf_nc/\"),\n",
    "parquet_train=Path(\"../data_processed/ground_train_h6.parquet\"),\n",
    "parquet_val=Path(\"../data_processed/ground_val_h6.parquet\"),\n",
    "parquet_test=Path(\"../data_processed/ground_test_h6.parquet\"),\n",
    "target_col='y_ghi_h6',\n",
    "aoi_bbox=(-81.5, -4.5, -66.0, 13.0), # Colombia-ish bbox; adjust\n",
    "seq_len=12,\n",
    "lead_steps=0,\n",
    "batch_size=8,\n",
    "epochs=40,\n",
    "lr=1e-3,\n",
    "out_dir=Path(\"../models\")\n",
    ")\n",
    "\n",
    "\n",
    "# Run once for DSRF\n",
    "print(\"\\n>>> Training fusion with DSRF…\")\n",
    "cfg.product = 'DSRF'\n",
    "train_fusion(cfg)\n",
    "\n",
    "\n",
    "# And once for MCMIPF (re-using the rest)\n",
    "print(\"\\n>>> Training fusion with MCMIPF…\")\n",
    "cfg.product = 'MCMIPF'\n",
    "train_fusion(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_lstm = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, n_features)),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "hist_lstm, yhat_lstm = fit_and_eval(mdl_lstm, \"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a313f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_bilstm = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, n_features)),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "hist_bilstm, yhat_bilstm = fit_and_eval(mdl_bilstm, \"BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_cnnlstm = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, n_features)),\n",
    "    layers.Conv1D(32, kernel_size=3, padding=\"causal\", activation=\"relu\"),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "hist_cnnlstm, yhat_cnnlstm = fit_and_eval(mdl_cnnlstm, \"CNN-LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3217e9d",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8febc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(y_true, preds_dict, n=500, title=\"Test — Truth vs Models (primeros puntos)\"):\n",
    "    sl = slice(0, min(n, len(y_true)))\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(y_true[sl], label=\"truth\", lw=1.2)\n",
    "    for name, yhat in preds_dict.items():\n",
    "        plt.plot(pd.Series(yhat, index=i_te)[sl], label=name, lw=1.0)\n",
    "    plt.title(title); plt.grid(True, ls=\"--\", alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ea626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isfinite\n",
    "plot_series(pd.Series(yte_seq, index=i_te),\n",
    "            {\"baseline\": ybase_seq,\n",
    "             \"RF\": yhat_te_rf.reindex(i_te).values,\n",
    "             \"LSTM\": yhat_lstm,\n",
    "             \"BiLSTM\": yhat_bilstm,\n",
    "             \"CNN-LSTM\": yhat_cnnlstm},\n",
    "            n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(y_true, y_pred, name):\n",
    "    plt.figure(figsize=(4.2,4))\n",
    "    plt.scatter(y_true, y_pred, s=8, alpha=0.4)\n",
    "    lim = [0, max(2.0, np.nanmax(y_true), np.nanmax(y_pred))]\n",
    "    plt.plot(lim, lim, \"k--\", lw=1)\n",
    "    plt.xlim(lim); plt.ylim(lim)\n",
    "    plt.xlabel(\"y_true (k)\"); plt.ylabel(\"y_pred (k)\")\n",
    "    plt.title(f\"Scatter Test — {name}\")\n",
    "    plt.grid(True, ls=\"--\", alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "scatter_plot(yte_seq, yhat_cnnlstm, \"CNN-LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee40155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist residual (CNN-LSTM)\n",
    "res = yhat_cnnlstm - yte_seq\n",
    "plt.figure(figsize=(6,3)); plt.hist(res, bins=40, alpha=0.85)\n",
    "plt.title(\"Residuals (y_pred - y_true) — CNN-LSTM (Test)\")\n",
    "plt.xlabel(\"residual\"); plt.ylabel(\"count\")\n",
    "plt.grid(True, ls=\"--\", alpha=0.3); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skill vs Baseline (secuencias)\n",
    "def rmse_np(a,b): return np.sqrt(np.mean((a-b)**2))\n",
    "rmse_base = rmse_np(yte_seq, ybase_seq)\n",
    "for name, yhat in [(\"RF\", yhat_te_rf.reindex(i_te).values),\n",
    "                   (\"LSTM\", yhat_lstm),\n",
    "                   (\"BiLSTM\", yhat_bilstm),\n",
    "                   (\"CNN-LSTM\", yhat_cnnlstm)]:\n",
    "    s = 1 - (rmse_np(yte_seq, yhat) / rmse_base)\n",
    "    print(f\"Skill (RMSE) vs baseline — {name}: {s:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e_ladino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
