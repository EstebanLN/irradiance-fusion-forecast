{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7811bfe6",
   "metadata": {},
   "source": [
    "# Fusion baseline Model (GOES + Ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fusion (NPZ) — GOES images + ground/tabular → GHI (simple & clean)\n",
    "\n",
    "Assumptions\n",
    "- Filenames encode UTC hour as YYYYMMDD_HH (e.g., 20220101_00_MCMIPF.npz).\n",
    "- DSRF .npz: [H,W] or [T,H,W] (use last along axis 0).\n",
    "- MCMIPF .npz: [C,H,W] or [T,H,W] (treated as channels).\n",
    "- Tabular will be coerced to UTC and downsampled to hourly.\n",
    "\n",
    "Paths\n",
    "- DSRF  : /mnt/SOLARLAB/E_Ladino/Repo/irradiance-fusion-forecast/data_processed/GOES/DSRF\n",
    "- MCMIPF: /mnt/SOLARLAB/E_Ladino/Repo/irradiance-fusion-forecast/data_processed/GOES/MCMIPF\n",
    "- Parquets: ../data_processed/ground_{train,val,test}_h6.parquet\n",
    "- Outputs : ../models + ../reports/figures\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ff1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import re, json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347cdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- CONFIG -----------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    product: str  # 'DSRF' or 'MCMIPF'\n",
    "    dsrf_dir: Path\n",
    "    mcmipf_dir: Path\n",
    "    parquet_train: Path\n",
    "    parquet_val: Path\n",
    "    parquet_test: Path\n",
    "    target_col: str = \"y_ghi_h6\"\n",
    "    feature_cols: Optional[List[str]] = None\n",
    "    seq_len: int = 12\n",
    "    batch_size: int = 8\n",
    "    epochs: int = 40\n",
    "    out_dir: Path = Path(\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- INDEX (NPZ) -----------------\n",
    "_RGX = re.compile(r\"(20\\d{6})[_T]?(\\d{2})\")  # YYYYMMDD_HH\n",
    "\n",
    "def ts_from_name(p: Path) -> Optional[pd.Timestamp]:\n",
    "    m = _RGX.search(p.name)\n",
    "    if not m:\n",
    "        return None\n",
    "    ymd, hh = m.groups()\n",
    "    return pd.to_datetime(ymd + hh + \"00\", format=\"%Y%m%d%H%M\", utc=True)\n",
    "\n",
    "def list_npz_by_ts(root: Path) -> pd.Series:\n",
    "    files = sorted(Path(root).glob(\"**/*.npz\"))\n",
    "    pairs = [(ts_from_name(p), p) for p in files]\n",
    "    pairs = [(t, p) for (t, p) in pairs if t is not None]\n",
    "    if not pairs:\n",
    "        raise FileNotFoundError(f\"No .npz with timestamp pattern under {root}\")\n",
    "    s = pd.Series({t: p for t, p in pairs}).sort_index()\n",
    "    s.index.name = \"ts\"\n",
    "    return s\n",
    "\n",
    "# ----------------- READERS (NPZ) -----------------\n",
    "def _normalize_dsrf(img: np.ndarray) -> np.ndarray:\n",
    "    img = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    img = np.clip(img, 0, None) / 1200.0  # ~[0,1]\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def _standardize_per_channel(chw: np.ndarray) -> np.ndarray:\n",
    "    chw = np.nan_to_num(chw, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    C, H, W = chw.shape\n",
    "    out = np.empty_like(chw, dtype=np.float32)\n",
    "    for c in range(C):\n",
    "        band = chw[c]\n",
    "        med = np.median(band)\n",
    "        p90 = np.percentile(band, 90)\n",
    "        scale = p90 if p90 > 1e-3 else 1.0\n",
    "        out[c] = (band - med) / scale\n",
    "    return out\n",
    "\n",
    "def read_dsrf_npz(npz_path: Path) -> np.ndarray:\n",
    "    data = np.load(npz_path)\n",
    "    key = \"dsrf\" if \"dsrf\" in data else list(data.keys())[0]\n",
    "    arr = data[key]\n",
    "    if arr.ndim == 2:\n",
    "        img = _normalize_dsrf(arr)\n",
    "        return img[..., None]                   # H,W,1\n",
    "    if arr.ndim == 3:\n",
    "        img = _normalize_dsrf(arr[-1])         # last frame\n",
    "        return img[..., None]\n",
    "    raise ValueError(f\"Unexpected DSRF shape: {arr.shape}\")\n",
    "\n",
    "def read_mcmipf_npz(npz_path: Path) -> np.ndarray:\n",
    "    data = np.load(npz_path)\n",
    "    key = \"mcmipf\" if \"mcmipf\" in data else list(data.keys())[0]\n",
    "    arr = data[key]\n",
    "    if arr.ndim == 3:                           # [C,H,W] or [T,H,W]\n",
    "        chw = _standardize_per_channel(arr.astype(np.float32))\n",
    "        return np.transpose(chw, (1, 2, 0))     # H,W,C\n",
    "    if arr.ndim == 2:\n",
    "        hw = arr.astype(np.float32)\n",
    "        med = np.median(hw); p90 = np.percentile(hw, 90); p90 = p90 if p90 > 1e-3 else 1.0\n",
    "        hw = (hw - med) / p90\n",
    "        return hw[..., None]\n",
    "    raise ValueError(f\"Unexpected MCMIPF shape: {arr.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- SEQUENCES -----------------\n",
    "def build_image_sequences(ts_index: pd.DatetimeIndex, fmap: pd.Series, L: int, product: str) -> Dict[pd.Timestamp, np.ndarray]:\n",
    "    if len(ts_index) < L:\n",
    "        return {}\n",
    "    freq = pd.to_timedelta(\"1H\")\n",
    "    out: Dict[pd.Timestamp, np.ndarray] = {}\n",
    "    for t in ts_index:\n",
    "        seq = pd.date_range(end=t, periods=L, freq=freq)\n",
    "        if not all(tt in fmap.index for tt in seq):\n",
    "            continue\n",
    "        frames = []\n",
    "        for tt in seq:\n",
    "            p = fmap.loc[tt]\n",
    "            hwc = read_dsrf_npz(p) if product.upper() == \"DSRF\" else read_mcmipf_npz(p)\n",
    "            frames.append(hwc)\n",
    "        out[t] = np.stack(frames, axis=0).astype(np.float32)  # [L,H,W,C]\n",
    "    return out\n",
    "\n",
    "def build_tabular_sequences(df: pd.DataFrame, target_col: str, L: int) -> Dict[pd.Timestamp, Tuple[np.ndarray,float]]:\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"{target_col} not found\")\n",
    "    feat_cols = [c for c in df.columns if c != target_col and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    X = df[feat_cols].astype(\"float32\")\n",
    "    y = df[target_col].astype(\"float32\")\n",
    "    out: Dict[pd.Timestamp, Tuple[np.ndarray,float]] = {}\n",
    "    idx = df.index\n",
    "    for i in range(L-1, len(df)):\n",
    "        t = idx[i]\n",
    "        block = X.iloc[i-L+1:i+1].values\n",
    "        if np.isnan(block).any() or not np.isfinite(y.iloc[i]):\n",
    "            continue\n",
    "        out[t] = (block, float(y.iloc[i]))\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- TF.DATA -----------------\n",
    "def make_tf_dataset(img_dict, tab_dict, batch: int, shuffle=True):\n",
    "    keys = sorted(set(img_dict.keys()) & set(tab_dict.keys()))\n",
    "    if not keys:\n",
    "        raise ValueError(\"No common timestamps between images and tabular.\")\n",
    "    Ximg = np.stack([img_dict[t] for t in keys])\n",
    "    Xtab = np.stack([tab_dict[t][0] for t in keys])\n",
    "    Y    = np.array([tab_dict[t][1] for t in keys], dtype=\"float32\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((Ximg, Xtab), Y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(min(4*batch, len(Y)))\n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds, keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f18245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- MODELS -----------------\n",
    "def build_convlstm_fusion(input_img, input_tab, dropout=0.2):\n",
    "    L,H,W,C = input_img; Lt,F = input_tab\n",
    "    img_in = layers.Input(shape=(L,H,W,C))\n",
    "    x = layers.ConvLSTM2D(32, (3,3), padding=\"same\", return_sequences=True, activation=\"relu\")(img_in)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ConvLSTM2D(32, (3,3), padding=\"same\", return_sequences=False, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    tab_in = layers.Input(shape=(Lt,F))\n",
    "    t = layers.LSTM(64)(tab_in)\n",
    "\n",
    "    h = layers.Concatenate()([x,t])\n",
    "    h = layers.Dropout(dropout)(h)\n",
    "    h = layers.Dense(128, activation=\"relu\")(h)\n",
    "    h = layers.Dense(64, activation=\"relu\")(h)\n",
    "    out = layers.Dense(1, dtype=\"float32\")(h)\n",
    "\n",
    "    m = models.Model([img_in, tab_in], out)\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return m\n",
    "\n",
    "def build_3dcnn_fusion(input_img, input_tab, dropout=0.2):\n",
    "    L,H,W,C = input_img; Lt,F = input_tab\n",
    "    img_in = layers.Input(shape=(L,H,W,C))\n",
    "    x = layers.Conv3D(32, (3,3,3), strides=(1,2,2), padding=\"same\", activation=\"relu\")(img_in)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv3D(64, (3,3,3), strides=(1,2,2), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "\n",
    "    tab_in = layers.Input(shape=(Lt,F))\n",
    "    t = layers.LSTM(64)(tab_in) if Lt > 1 else layers.Flatten()(tab_in)\n",
    "    t = layers.Dense(128, activation=\"relu\")(t)\n",
    "\n",
    "    h = layers.Concatenate()([x,t])\n",
    "    h = layers.Dropout(dropout)(h)\n",
    "    h = layers.Dense(128, activation=\"relu\")(h)\n",
    "    h = layers.Dense(64, activation=\"relu\")(h)\n",
    "    out = layers.Dense(1, dtype=\"float32\")(h)\n",
    "\n",
    "    m = models.Model([img_in, tab_in], out)\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb78aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- TRAIN / EVAL -----------------\n",
    "def _ensure_utc_hourly(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    idx = df.index\n",
    "    if getattr(idx, \"tz\", None) is None:\n",
    "        df = df.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        df = df.tz_convert(\"UTC\")\n",
    "    return df.groupby(pd.Grouper(freq=\"1H\")).last().dropna(how=\"any\")\n",
    "\n",
    "def train_fusion(cfg: Config):\n",
    "    cfg.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Tabular → UTC hourly\n",
    "    tr = _ensure_utc_hourly(pd.read_parquet(cfg.parquet_train).sort_index())\n",
    "    va = _ensure_utc_hourly(pd.read_parquet(cfg.parquet_val).sort_index())\n",
    "    te = _ensure_utc_hourly(pd.read_parquet(cfg.parquet_test).sort_index())\n",
    "\n",
    "    feat_cols = cfg.feature_cols or [c for c in tr.columns if c != cfg.target_col and pd.api.types.is_numeric_dtype(tr[c])]\n",
    "    tr = tr[feat_cols + [cfg.target_col]]\n",
    "    va = va[feat_cols + [cfg.target_col]]\n",
    "    te = te[feat_cols + [cfg.target_col]]\n",
    "\n",
    "    # NPZ images → restrict to tabular time span\n",
    "    fmap = list_npz_by_ts(cfg.dsrf_dir if cfg.product.upper()==\"DSRF\" else cfg.mcmipf_dir)\n",
    "    tmin = min(tr.index.min(), va.index.min()); tmax = te.index.max()\n",
    "    fmap = fmap[(fmap.index >= tmin) & (fmap.index <= tmax)]\n",
    "\n",
    "    # Build sequences on intersection\n",
    "    img_tr = build_image_sequences(fmap.index[fmap.index.isin(tr.index)], fmap, cfg.seq_len, cfg.product)\n",
    "    img_va = build_image_sequences(fmap.index[fmap.index.isin(va.index)], fmap, cfg.seq_len, cfg.product)\n",
    "    img_te = build_image_sequences(fmap.index[fmap.index.isin(te.index)], fmap, cfg.seq_len, cfg.product)\n",
    "\n",
    "    tab_tr = build_tabular_sequences(tr, cfg.target_col, cfg.seq_len)\n",
    "    tab_va = build_tabular_sequences(va, cfg.target_col, cfg.seq_len)\n",
    "    tab_te = build_tabular_sequences(te, cfg.target_col, cfg.seq_len)\n",
    "\n",
    "    ds_tr, keys_tr = make_tf_dataset(img_tr, tab_tr, cfg.batch_size, shuffle=True)\n",
    "    ds_va, keys_va = make_tf_dataset(img_va, tab_va, cfg.batch_size, shuffle=False)\n",
    "    ds_te, keys_te = make_tf_dataset(img_te, tab_te, cfg.batch_size, shuffle=False)\n",
    "\n",
    "    # Shapes\n",
    "    (ximg_s, xtab_s), _ = next(iter(ds_tr.take(1)))\n",
    "    L,H,W,C = ximg_s.shape[1:]; Lt,F = xtab_s.shape[1:]\n",
    "\n",
    "    # Model\n",
    "    model = build_convlstm_fusion((L,H,W,C), (Lt,F)) if cfg.product.upper()==\"DSRF\" else \\\n",
    "            build_3dcnn_fusion((L,H,W,C), (Lt,F))\n",
    "\n",
    "    ckpt = str((cfg.out_dir / f\"best_{cfg.product.lower()}_fusion.keras\").resolve())\n",
    "    cbs = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(ckpt, monitor=\"val_loss\", save_best_only=True),\n",
    "    ]\n",
    "\n",
    "    model.fit(ds_tr, validation_data=ds_va, epochs=cfg.epochs, callbacks=cbs)\n",
    "\n",
    "    # Evaluate\n",
    "    y_true, y_pred = [], []\n",
    "    for (xi, xt), y in ds_te:\n",
    "        yp = model.predict((xi, xt), verbose=0)\n",
    "        y_true.append(y.numpy()); y_pred.append(yp)\n",
    "    y_true = np.concatenate(y_true).squeeze()\n",
    "    y_pred = np.concatenate(y_pred).squeeze()\n",
    "\n",
    "    # Baseline\n",
    "    base_src = None\n",
    "    for c in [\"ghi_qc\",\"ghi_sg_definitive\",\"ghi_qc_lag1\"]:\n",
    "        if c in te.columns: base_src = te[c]; break\n",
    "    if base_src is None:\n",
    "        base_src = pd.Series(np.nanmedian(tr[cfg.target_col]), index=te.index)\n",
    "\n",
    "    idx_te = pd.DatetimeIndex(keys_te)\n",
    "    y_base = base_src.reindex(idx_te).to_numpy(dtype=\"float32\")\n",
    "\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "    mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse_base = float(np.sqrt(np.mean((y_true - y_base[:len(y_true)])**2)))\n",
    "    skill = 1.0 - (rmse / (rmse_base if rmse_base > 1e-9 else rmse))\n",
    "\n",
    "    out = {\"product\": cfg.product, \"seq_len\": int(cfg.seq_len),\n",
    "           \"rmse\": rmse, \"mae\": mae, \"skill\": float(skill), \"n_test\": int(len(y_true))}\n",
    "    with open(cfg.out_dir / f\"metrics_{cfg.product.lower()}.json\", \"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "\n",
    "    # Summary CSV\n",
    "    summary_csv = cfg.out_dir / \"fusion_test_summary.csv\"\n",
    "    pd.DataFrame([out]).to_csv(summary_csv, mode=\"a\" if summary_csv.exists() else \"w\",\n",
    "                               index=False, header=not summary_csv.exists())\n",
    "\n",
    "    # Plots\n",
    "    OUT_FIG = Path(\"../reports/figures\"); OUT_FIG.mkdir(parents=True, exist_ok=True)\n",
    "    N = min(400, len(y_true))\n",
    "\n",
    "    plt.figure(figsize=(12, 3.6))\n",
    "    plt.plot(idx_te[:N], y_true[:N], label=\"truth\", lw=1.4)\n",
    "    plt.plot(idx_te[:N], y_pred[:N], label=f\"{cfg.product} fusion\", lw=1.1)\n",
    "    plt.plot(idx_te[:N], y_base[:N], label=\"baseline\", lw=1.0, alpha=0.7)\n",
    "    plt.title(f\"Test — Truth vs Fusion ({cfg.product}) vs Baseline ({cfg.target_col})\")\n",
    "    plt.ylabel(\"GHI (W/m²)\" if cfg.target_col.startswith(\"y_ghi\") else \"target\")\n",
    "    plt.xlabel(\"Time\"); plt.grid(True, ls=\"--\", alpha=0.3); plt.legend()\n",
    "    plt.xticks(rotation=45); plt.tight_layout()\n",
    "    plt.savefig(OUT_FIG / f\"{cfg.product}_fusion_ts_test.png\", dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "    lim_min = float(min(np.min(y_true), np.min(y_pred)))\n",
    "    lim_max = float(max(np.max(y_true), np.max(y_pred)))\n",
    "    plt.figure(figsize=(4.8, 4.8))\n",
    "    plt.scatter(y_true, y_pred, s=10, alpha=0.5)\n",
    "    plt.plot([lim_min, lim_max], [lim_min, lim_max], linestyle=\"--\", linewidth=1.0)\n",
    "    plt.xlabel(\"Actual\"); plt.ylabel(\"Predicted\")\n",
    "    plt.title(f\"{cfg.product} Fusion — Actual vs Predicted\\nRMSE={rmse:.3f}  MAE={mae:.3f}  Skill={skill:.3f}\")\n",
    "    plt.grid(True, ls=\"--\", alpha=0.3); plt.tight_layout()\n",
    "    plt.savefig(OUT_FIG / f\"{cfg.product}_fusion_scatter.png\", dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "    resid = y_pred - y_true\n",
    "    plt.figure(figsize=(6, 3.2))\n",
    "    plt.hist(resid, bins=50, alpha=0.85)\n",
    "    plt.axvline(0, linestyle=\"--\", linewidth=1)\n",
    "    plt.title(f\"{cfg.product} Fusion — Residuals (mean={np.mean(resid):.3f})\")\n",
    "    plt.xlabel(\"Residual\"); plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True, ls=\"--\", alpha=0.3); plt.tight_layout()\n",
    "    plt.savefig(OUT_FIG / f\"{cfg.product}_fusion_residuals.png\", dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "    print(json.dumps(out, indent=2))\n",
    "    return model, out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81079e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- MAIN -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config(\n",
    "        product=\"DSRF\",\n",
    "        dsrf_dir=Path(\"/mnt/SOLARLAB/E_Ladino/Repo/irradiance-fusion-forecast/data_processed/GOES/DSRF\"),\n",
    "        mcmipf_dir=Path(\"/mnt/SOLARLAB/E_Ladino/Repo/irradiance-fusion-forecast/data_processed/GOES/MCMIPF\"),\n",
    "        parquet_train=Path(\"../data_processed/ground_train_h6.parquet\"),\n",
    "        parquet_val=Path(\"../data_processed/ground_val_h6.parquet\"),\n",
    "        parquet_test=Path(\"../data_processed/ground_test_h6.parquet\"),\n",
    "        seq_len=12, batch_size=8, epochs=40, out_dir=Path(\"../models\")\n",
    "    )\n",
    "\n",
    "    print(\"\\n>>> Training fusion with DSRF (NPZ, hourly UTC)…\")\n",
    "    cfg.product = \"DSRF\"\n",
    "    train_fusion(cfg)\n",
    "\n",
    "    print(\"\\n>>> Training fusion with MCMIPF (NPZ, hourly UTC)…\")\n",
    "    cfg.product = \"MCMIPF\"\n",
    "    train_fusion(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e_ladino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
