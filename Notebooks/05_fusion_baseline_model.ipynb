{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7811bfe6",
   "metadata": {},
   "source": [
    "# Fusion baseline Model (GOES + Ground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616708f3",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9e8c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 07:38:43.120238: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-11 07:38:43.125691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757594323.131939  336635 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757594323.134004  336635 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-11 07:38:43.141095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80442702",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae382d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_train = Path(\"../data_processed/ground_train_h6.parquet\")\n",
    "ground_val   = Path(\"../data_processed/ground_val_h6.parquet\")\n",
    "ground_test  = Path(\"../data_processed/ground_test_h6.parquet\")\n",
    "\n",
    "sat_features = Path(\"../data_interim/goes_dataset/goes_features.parquet\")  # generado en tu pipeline\n",
    "target_col   = \"y_k_h6\"\n",
    "FREQ         = \"10min\"\n",
    "SEQ_LEN      = 12 # 12*10min = 2 horas de contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3836198",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79e4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground: (57789, 41) (12384, 41) (12384, 41)  | Sat: (300, 32)\n"
     ]
    }
   ],
   "source": [
    "g_tr = pd.read_parquet(ground_train)\n",
    "g_va = pd.read_parquet(ground_val)\n",
    "g_te = pd.read_parquet(ground_test)\n",
    "\n",
    "for g in (g_tr, g_va, g_te):\n",
    "    idx = pd.to_datetime(g.index)\n",
    "    g.index = idx.tz_localize(\"UTC\") if idx.tz is None else idx.tz_convert(\"UTC\")\n",
    "\n",
    "sat = pd.read_parquet(sat_features)\n",
    "sat.index = pd.to_datetime(sat.index)\n",
    "sat.index = sat.index.tz_localize(\"UTC\") if sat.index.tz is None else sat.index.tz_convert(\"UTC\")\n",
    "sat = sat.select_dtypes(include=[np.number])  # sólo numérico\n",
    "sat.index = sat.index.round(FREQ)\n",
    "sat = sat.groupby(sat.index).mean().sort_index()\n",
    "\n",
    "print(\"Ground:\", g_tr.shape, g_va.shape, g_te.shape, \" | Sat:\", sat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PreDiagnosis ===\n",
      "Ground test shape: (12384, 41)\n",
      "Sat features shape: (300, 32)\n",
      "Ground test range: 2025-01-01 04:00:00+00:00 -> 2025-03-28 03:50:00+00:00\n",
      "Sat features range: 2024-02-01 00:30:00+00:00 -> 2024-02-10 15:10:00+00:00\n",
      "Columnas satelitales en sat: 32\n",
      "Columnas satelitales en Xte: 0\n",
      "Columnas en train pero no en test: 32\n",
      "Son: ['C13_std', 'C15_std', 'C06_std', 'C07_mean', 'C01_mean', 'C01_std', 'C09_mean', 'C07_std', 'C12_mean', 'C12_std', 'C05_mean', 'C11_mean', 'C16_mean', 'C13_mean', 'C04_std', 'C14_mean', 'C09_std', 'C03_mean', 'C04_mean', 'C08_mean', 'C16_std', 'C14_std', 'C08_std', 'C10_std', 'C06_mean', 'C05_std', 'C11_std', 'C02_mean', 'C02_std', 'C10_mean', 'C15_mean', 'C03_std']\n"
     ]
    }
   ],
   "source": [
    "# print(\"=== PreDiagnosis ===\")\n",
    "# print(\"Ground test shape:\", g_te.shape)\n",
    "# print(\"Sat features shape:\", sat.shape)\n",
    "# print(\"Ground test range:\", g_te.index.min(), \"->\", g_te.index.max())\n",
    "# print(\"Sat features range:\", sat.index.min(), \"->\", sat.index.max())\n",
    "\n",
    "# # Verifica si las columnas satelitales existen\n",
    "# sat_cols = [c for c in sat.columns if 'mean' in c or 'std' in c]\n",
    "# print(f\"Columnas satelitales en sat: {len(sat_cols)}\")\n",
    "# print(f\"Columnas satelitales en Xte: {len([c for c in Xte.columns if c in sat_cols])}\")\n",
    "\n",
    "# # Muestra las columnas faltantes\n",
    "# missing_in_test = set(Xtr.columns) - set(Xte.columns)\n",
    "# print(f\"Columnas en train pero no en test: {len(missing_in_test)}\")\n",
    "# if missing_in_test:\n",
    "#     print(\"Son:\", list(missing_in_test))#[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb92988",
   "metadata": {},
   "source": [
    "## Join & cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996216bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined — train: (1440, 72) val: (12384, 40) test: (12384, 40)\n"
     ]
    }
   ],
   "source": [
    "def prepare_split(gdf, sat_df, target):\n",
    "    df = gdf.join(sat_df, how=\"left\")\n",
    "    df = df.dropna(subset=[target])\n",
    "\n",
    "    # features = todo lo numérico excepto targets\n",
    "    X = (\n",
    "        df.drop(columns=[c for c in df.columns if c.startswith(\"y_\") or c == target], errors=\"ignore\")\n",
    "          .select_dtypes(include=[np.number])\n",
    "          .astype(\"float32\")\n",
    "    )\n",
    "    y = df[target].astype(\"float32\")\n",
    "\n",
    "    # ffill/bfill por día para reducir huecos de sat\n",
    "    X = X.groupby(X.index.date).apply(lambda d: d.ffill().bfill()).reset_index(level=0, drop=True)\n",
    "    # quita columnas vacías\n",
    "    X = X.dropna(axis=1, how=\"all\")\n",
    "    # filtra filas con demasiados NaN\n",
    "    keep = (X.isna().mean(axis=1) <= 0.3)\n",
    "    X, y = X.loc[keep], y.loc[keep]\n",
    "    # relleno final mínimo\n",
    "    X = X.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "Xtr, ytr = prepare_split(g_tr, sat, target_col)\n",
    "Xva, yva = prepare_split(g_va, sat, target_col)\n",
    "Xte, yte = prepare_split(g_te, sat, target_col)\n",
    "\n",
    "print(\"Joined — train:\", Xtr.shape, \"val:\", Xva.shape, \"test:\", Xte.shape)\n",
    "if min(len(Xtr), len(Xva), len(Xte)) == 0:\n",
    "    raise RuntimeError(\"❌ No quedaron filas tras el join/limpieza. Revisa cobertura temporal de sat/ground.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada9bfc",
   "metadata": {},
   "source": [
    "## Baseline (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a0e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline → RMSE: 72215650304.0  MAE: 42850.25390625\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- C01_mean\n- C01_std\n- C02_mean\n- C02_std\n- C03_mean\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m rf = RandomForestRegressor(n_estimators=\u001b[32m400\u001b[39m, random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m     11\u001b[39m rf.fit(Xtr, ytr)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m yhat_te_rf = pd.Series(\u001b[43mrf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXte\u001b[49m\u001b[43m)\u001b[49m, index=Xte.index)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRF Test → RMSE:\u001b[39m\u001b[33m\"\u001b[39m, rmse(yte, yhat_te_rf), \u001b[33m\"\u001b[39m\u001b[33m MAE:\u001b[39m\u001b[33m\"\u001b[39m, mean_absolute_error(yte, yhat_te_rf), \u001b[33m\"\u001b[39m\u001b[33m R2:\u001b[39m\u001b[33m\"\u001b[39m, r2_score(yte, yhat_te_rf))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:1065\u001b[39m, in \u001b[36mForestRegressor.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1063\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m   1068\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/sklearn/utils/validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/sklearn/utils/validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- C01_mean\n- C01_std\n- C02_mean\n- C02_std\n- C03_mean\n- ...\n"
     ]
    }
   ],
   "source": [
    "def rmse(a,b): return mean_squared_error(a,b)\n",
    "\n",
    "if \"k_ghi_lag1\" in Xte.columns:\n",
    "    yhat_base = Xte[\"k_ghi_lag1\"].clip(0, 2.0)\n",
    "else:\n",
    "    yhat_base = pd.Series(np.median(ytr), index=yte.index)\n",
    "\n",
    "print(\"Baseline → RMSE:\", rmse(yte, yhat_base), \" MAE:\", mean_absolute_error(yte, yhat_base))\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)\n",
    "rf.fit(Xtr, ytr)\n",
    "yhat_te_rf = pd.Series(rf.predict(Xte), index=Xte.index)\n",
    "print(\"RF Test → RMSE:\", rmse(yte, yhat_te_rf), \" MAE:\", mean_absolute_error(yte, yhat_te_rf), \" R2:\", r2_score(yte, yhat_te_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1938607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sat.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6152f1f",
   "metadata": {},
   "source": [
    "## Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c891a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m rows, times = [], []\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ts, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43msat\u001b[49m.iterrows():\n\u001b[32m      3\u001b[39m     out = {}\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m channels:\n",
      "\u001b[31mNameError\u001b[39m: name 'sat' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "Xtr_s = pd.DataFrame(scaler.fit_transform(Xtr), index=Xtr.index, columns=Xtr.columns)\n",
    "Xva_s = pd.DataFrame(scaler.transform(Xva), index=Xva.index, columns=Xva.columns)\n",
    "Xte_s = pd.DataFrame(scaler.transform(Xte), index=Xte.index, columns=Xte.columns)\n",
    "\n",
    "def build_sequences(X_df, y_ser, seq_len):\n",
    "    Xv, yv = X_df.values, y_ser.values\n",
    "    n = len(X_df)\n",
    "    xs, ys, idxs = [], [], []\n",
    "    for i in range(seq_len-1, n):\n",
    "        # asumimos grilla regular 10 min; si quieres, verifica gaps aquí\n",
    "        block = Xv[i-seq_len+1:i+1]\n",
    "        if np.isnan(block).any():\n",
    "            continue\n",
    "        xs.append(block)\n",
    "        ys.append(yv[i])\n",
    "        idxs.append(X_df.index[i])\n",
    "    return np.array(xs, dtype=\"float32\"), np.array(ys, dtype=\"float32\"), pd.DatetimeIndex(idxs)\n",
    "\n",
    "Xtr_seq, ytr_seq, i_tr = build_sequences(Xtr_s, ytr, SEQ_LEN)\n",
    "Xva_seq, yva_seq, i_va = build_sequences(Xva_s, yva, SEQ_LEN)\n",
    "Xte_seq, yte_seq, i_te = build_sequences(Xte_s, yte, SEQ_LEN)\n",
    "\n",
    "n_features = Xtr_seq.shape[2]\n",
    "print(\"Seq shapes →\",\n",
    "      \"Xtr\", Xtr_seq.shape, \"Xva\", Xva_seq.shape, \"Xte\", Xte_seq.shape, \"| features:\", n_features)\n",
    "\n",
    "if min(len(Xtr_seq), len(Xva_seq), len(Xte_seq)) == 0:\n",
    "    raise RuntimeError(\"❌ No hay suficientes ventanas para las secuencias. Reduce SEQ_LEN o revisa cobertura.\")\n",
    "\n",
    "# Alinear baseline a las muestras secuenciales del test\n",
    "ybase_seq = yhat_base.reindex(i_te).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca57f39",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03cddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined shapes — train: (40344, 40) val: (12360, 40) test: (0, 40)\n"
     ]
    }
   ],
   "source": [
    "es = callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_loss\")\n",
    "\n",
    "def fit_and_eval(model, name, epochs=60, batch=256):\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.MAE])\n",
    "    hist = model.fit(Xtr_seq, ytr_seq,\n",
    "                     validation_data=(Xva_seq, yva_seq),\n",
    "                     epochs=epochs, batch_size=batch,\n",
    "                     verbose=0, callbacks=[es])\n",
    "    # predicciones\n",
    "    yhat_va = model.predict(Xva_seq, verbose=0).squeeze()\n",
    "    yhat_te = model.predict(Xte_seq, verbose=0).squeeze()\n",
    "    # métricas\n",
    "    def rmse_np(a,b): return np.sqrt(np.mean((a-b)**2))\n",
    "    print(f\"{name} → Val RMSE: {rmse_np(yva_seq, yhat_va):.4f} | Test RMSE: {rmse_np(yte_seq, yhat_te):.4f} \"\n",
    "          f\"| Test MAE: {mean_absolute_error(yte_seq, yhat_te):.4f} | R2: {r2_score(yte_seq, yhat_te):.4f}\")\n",
    "    return hist, yhat_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_lstm = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, n_features)),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "hist_lstm, yhat_lstm = fit_and_eval(mdl_lstm, \"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a313f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_bilstm = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, n_features)),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "hist_bilstm, yhat_bilstm = fit_and_eval(mdl_bilstm, \"BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_cnnlstm = models.Sequential([\n",
    "    layers.Input(shape=(SEQ_LEN, n_features)),\n",
    "    layers.Conv1D(32, kernel_size=3, padding=\"causal\", activation=\"relu\"),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "hist_cnnlstm, yhat_cnnlstm = fit_and_eval(mdl_cnnlstm, \"CNN-LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3217e9d",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8febc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(y_true, preds_dict, n=500, title=\"Test — Truth vs Models (primeros puntos)\"):\n",
    "    sl = slice(0, min(n, len(y_true)))\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(y_true[sl], label=\"truth\", lw=1.2)\n",
    "    for name, yhat in preds_dict.items():\n",
    "        plt.plot(pd.Series(yhat, index=i_te)[sl], label=name, lw=1.0)\n",
    "    plt.title(title); plt.grid(True, ls=\"--\", alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ea626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isfinite\n",
    "plot_series(pd.Series(yte_seq, index=i_te),\n",
    "            {\"baseline\": ybase_seq,\n",
    "             \"RF\": yhat_te_rf.reindex(i_te).values,\n",
    "             \"LSTM\": yhat_lstm,\n",
    "             \"BiLSTM\": yhat_bilstm,\n",
    "             \"CNN-LSTM\": yhat_cnnlstm},\n",
    "            n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(y_true, y_pred, name):\n",
    "    plt.figure(figsize=(4.2,4))\n",
    "    plt.scatter(y_true, y_pred, s=8, alpha=0.4)\n",
    "    lim = [0, max(2.0, np.nanmax(y_true), np.nanmax(y_pred))]\n",
    "    plt.plot(lim, lim, \"k--\", lw=1)\n",
    "    plt.xlim(lim); plt.ylim(lim)\n",
    "    plt.xlabel(\"y_true (k)\"); plt.ylabel(\"y_pred (k)\")\n",
    "    plt.title(f\"Scatter Test — {name}\")\n",
    "    plt.grid(True, ls=\"--\", alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "scatter_plot(yte_seq, yhat_cnnlstm, \"CNN-LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee40155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist residual (CNN-LSTM)\n",
    "res = yhat_cnnlstm - yte_seq\n",
    "plt.figure(figsize=(6,3)); plt.hist(res, bins=40, alpha=0.85)\n",
    "plt.title(\"Residuals (y_pred - y_true) — CNN-LSTM (Test)\")\n",
    "plt.xlabel(\"residual\"); plt.ylabel(\"count\")\n",
    "plt.grid(True, ls=\"--\", alpha=0.3); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skill vs Baseline (secuencias)\n",
    "def rmse_np(a,b): return np.sqrt(np.mean((a-b)**2))\n",
    "rmse_base = rmse_np(yte_seq, ybase_seq)\n",
    "for name, yhat in [(\"RF\", yhat_te_rf.reindex(i_te).values),\n",
    "                   (\"LSTM\", yhat_lstm),\n",
    "                   (\"BiLSTM\", yhat_bilstm),\n",
    "                   (\"CNN-LSTM\", yhat_cnnlstm)]:\n",
    "    s = 1 - (rmse_np(yte_seq, yhat) / rmse_base)\n",
    "    print(f\"Skill (RMSE) vs baseline — {name}: {s:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e_ladino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
